# Difine libraries
from collections import defaultdict, OrderedDict
from urllib.parse import urlsplit
from tqdm import tqdm
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn import model_selection
from sklearn import svm
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, ExtraTreesClassifier,BaggingClassifier,GradientBoostingClassifier
import graphviz 
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.naive_bayes import BernoulliNB
import csv
import os, os.path
import pickle
import random
import time
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
# from mlxtend.classifier import EnsembleVoteClassifier,StackingCVClassifier,StackingClassifier
from vecstack import stacking
from xgboost import XGBClassifier
# Set seed for random numbers
random.seed(1)
# Define folder where the log files are located
folder = 'drebin/feature_vectors/'

# Define list for the malware and non malware files
non_malware = list()
malware = list()
# List of all the files

dataset = os.listdir(folder)
# Create malware dictionary with the file name and the type of each one
with open('drebin/sha256_family.csv') as csvfile:
	reader = csv.reader(csvfile)
	next(reader)
	malware_dictionary = {row[0]:row[1] for row in reader}
	# Separate the file names between malware and non malware
for i in dataset:
	if i in malware_dictionary:
		malware.append(i)
	else:
		non_malware.append(i)
print('Size of dataset: ', len(malware) + len(non_malware))
print('Number of non malwares:\t', len(non_malware))
print('Number of malwares:\t', len(malware))

# Generate random numbers
index = random.sample(range(0, len(non_malware)-1), len(malware))
# New list with malware and non malware examples divided
non_malware = [non_malware[i] for i in index]
# Merged list containing malware and non malware
data = malware + non_malware
# Vector with class of each example
y = [1]*len(malware) + [0]*len(non_malware)
# Print number of examples in each class
print('Number of non malware: ', len(non_malware))
print('Number of malware: ', len(malware))

print('\nNumber of entries in each class of malware (values above 10):')
v = defaultdict(list)
for key, value in sorted(malware_dictionary.items()):
	v[value].append(key)
ordered_v = \
	OrderedDict(sorted(v.items(), key=lambda x: len(x[1]), reverse=True))
count_malware = 0
for k in ordered_v:
	if len(v[k])>10: # Print only classes with more than 20 examples
		count_malware += len(v[k])
		print('\t', '{:>3}'.format(len(v[k])), k)
print('\t', '{:>3}'.format(len(malware) - count_malware), 'Others')

# Collect all the features found in the first 20 files of the dataset
category_list = list()
for file in dataset[0:20]:
	with open(folder + file) as f:
		content = f.readlines()
		for line in content:
			category, string = line.split('::')
			if category not in category_list:
				category_list.append(category)
print('The features can be divided in', len(category_list), 'different sets:')
print('\t','\n\t '.join(category_list))

features = {
	'api_call': True,
	'feature': True,
	'url': True,
	'service_receiver': True,
	'permission': True,
	'call': True,
	'intent': True,
	'real_permission': True,
	'activity': True,
	'provider': True,
}

# Extract url
def extract_url(string):
	try:
		base_url = "{0.scheme}://{0.netloc}/".format(urlsplit(string))
		if len(base_url) > 10:
			return [base_url]
	except:
		#print('Error html: ', string)
		return None
# Extract api_call
def extract_api_call(string):
	try:
		string = string.replace(';->', '/')
		api_call = string.split('/')
		return api_call
	except:
		return None
# Extract feature
def extract_feature(string):
	try:
		feature = string.split('.')[-1]
		return [feature]
	except:
		return None
# Extract permission and real_permission
def extract_permission(string):
	try:
		permission = string.split('.')[-1].lower()
		return [permission]
	except:
		return None
# Extract call
def extract_call(string):
	try:
		call = string.lower()
		return [call]
	except:
		return None
# Extract activity
def extract_activity(string):
	try:
		activity = string.split('.')[-1].lower()
		return [activity]
	except:
		return None
# Extract intent
def extract_intent(string):
	try:
		intent = string.split('.')[-1].lower()
		return [intent]
	except:
		return None
# Extract service_receiver
def extract_service_receiver(string):
	try:
		service_receiver = string.split('.')[-1].lower()
		return [service_receiver]
	except:
		return None
# Extract provider
def extract_provider(string):
	try:
		provider = string.split('.')[-1].lower()
		return [provider]
	except:
		return None
# Create dictionary or list of words
def process_file(file, dictionary_creation = False):
	# List of words of each file
	words = list()
	# Read line by line of the file
	with open(folder + file) as f:
		content = f.readlines()
		# Divide each line of document in
		for line in content:
			try:
				split = line.split('::')
				category = split[0]
				string = split[1]
			except:
				break
			# Only process the categories selected by the user
			if (features[category]):
				if (category == 'url'):
					word_list = extract_url(string)
				elif (category == 'api_call'):
					word_list = extract_api_call(string)
				elif (category == 'feature'):
					word_list = extract_feature(string)
				elif (category == 'permission' or \
					category == 'real_permission'):
					word_list = extract_permission(string)
				elif (category == 'call'):
					word_list = extract_call(string)
				elif (category == 'activity'):
					word_list = extract_activity(string)
				elif (category == 'intent'):
					word_list = extract_intent(string)
				elif (category == 'service_receiver'):
					word_list = extract_service_receiver(string)
				elif (category == 'provider'):
					word_list = extract_provider(string)
				# If able to extract feature from line
				if word_list != None:
					for word in word_list:
						word = word.replace('\n', '')
						# If flagged to create the dictionary
						if dictionary_creation:
							index = len(dictionary)-1
							dictionary[word] = index
						else:
							index = dictionary[word]
							if index not in words:
								words.append(index)
		return words

# Save dictionary to file
def save_dic(obj, name):
	with open(name + '.pkl', 'wb') as f:
		pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)
# Load dictionary from file
def load_dic(name):
	with open(name + '.pkl', 'rb') as f:
		return pickle.load(f)
# Check if dictionary exists
if os.path.isfile('dictionary.pkl'):
	print('Dictionary file found!')
	dictionary = load_dic('dictionary')
else:
	print('Dictionary file not found!')
	# Define the dictionary
	dictionary = {}
	# Colect words for malware
	pbar = tqdm(range(len(data)))
	pbar.set_description('Creating dictionary')
	for i in pbar:
		process_file(data[i], True)
	# Save dictionary
	save_dic(dictionary, 'dictionary')
	print('\nDictionary saved to file!')
# Print number of words in the dictionary
dictionary_size = len(dictionary)
print('Dictionary size: ', dictionary_size)


# Construct the vector of features
def features_extraction(file):
	indices = process_file(file)
	feat = [0] * dictionary_size
	for i in indices:
		feat[i-1] = 1
	return feat
# List of features
X = list()
# Extract features and append to the list of features
pbar = tqdm(range(len(data)))
pbar.set_description('Extracting features')
for i in pbar:
	feat = features_extraction(data[i])
	X.append(feat)



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)


# Function to calculate the metrics
def metrics(y_test, y_predict):
	# Number of examples
	n = len(y_test)
	# Calculate true positives
	true_positive = \
	sum([y_test[i] and y_predict[i] for i in range(n)])
	print('\nTrue positive', true_positive)
	# Calculate false positives
	false_positive = \
	sum([not(y_test[i]) and y_predict[i] for i in range(n)])
	print('False positive', false_positive)
	# Calculate false negatives
	false_negative = \
	sum([y_test[i] and not(y_predict[i]) for i in range(n)])
	print('False negative', false_negative)
	# Calculate true negatives
	true_negative = \
	sum([not(y_test[i]) and not(y_predict[i]) for i in range(n)])
	print('True Negative', true_negative)
	# Calculate precision
	precision = true_positive/(true_positive + false_positive)
	print('\nPrecision', round(precision*100,2), '%')
	# Calculate recall
	recall = true_positive/(true_positive + false_negative)
	print('Recall', round(recall*100,2), '%')
	# Calculate false positive rate
	false_pos_rate = false_positive/(false_positive + true_negative)
	print('False positive rate', round(false_pos_rate*100,2), '%')
	# Calculate accuracy
	accuracy = (true_positive + true_negative)/ \
	(false_positive + false_negative + \
	true_positive + true_negative)
	print('Accuracy', round(accuracy*100,2), '%')
	# Calculate accuracy
	f_measure = 2 *(precision * recall)/(precision + recall)
	print('F-measure', round(f_measure*100,2), '%')

models = [
	    ExtraTreesClassifier(),
	        
	    RandomForestClassifier()
	    ]
	    
	# Compute stacking features
S_train, S_test = stacking(models, X_train, y_train, X_test, 
	    regression = False, metric = accuracy_score, n_folds = 5, 
	    stratified = True, shuffle = True, random_state = 0, verbose = 2)
print(S_train.shape)
print(S_test.shape)



# Initialize 2nd level model
model = XGBClassifier()
# vect.fit(S_train)
# S_train_dtm = vect.transform(S_train).todense()

#df_train = pd.DataFrame(vect.transform(X_train).todense())
#print(X_train_dtm)
# equivalently: combine fit and transform into a single step
# this is faster and what most people would do
# S_train_dtm = vect.fit_transform(S_train)

# Fit 2nd level model
model = model.fit(S_train, y_train)

# Predict
y_pred = model.predict(S_test)

# # Final prediction score
print('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))
# y_predict_dt = train_decision_tree_classifer(X_train, X_test, y_train)
# print(confusion_matrix(y_test, y_predict_dt))
# metrics(y_test, y_predict_dt)
# print(classification_report(y_test, y_predict_dt))

# # y_predict = train_svm_classifer(X_train, X_test, y_train)
# # metrics(y_test, y_predict)

# y_predict_et = train_et_classifer(X_train, X_test, y_train)
# print(confusion_matrix(y_test, y_predict_et))
# metrics(y_test, y_predict_et)
# print(classification_report(y_test, y_predict_et))

# y_predict_rf = train_rf_classifer(X_train, X_test, y_train)
# print(confusion_matrix(y_test, y_predict_rf))
# metrics(y_test, y_predict_rf)
# print(classification_report(y_test, y_predict_rf))

# y_predict_gb = train_gb_classifer(X_train, X_test, y_train)
# print(confusion_matrix(y_test, y_predict_gb))
# metrics(y_test, y_predict_gb)
# print(classification_report(y_test, y_predict_gb))
'''
clf1 = tree.DecisionTreeClassifier()
clf2 = ExtraTreesClassifier()
clf3 = RandomForestClassifier()
clf4 = GradientBoostingClassifier()

rf = BaggingClassifier()

sclf = StackingClassifier(classifiers=[clf1, clf2, clf3, clf4], 
                          meta_classifier=rf)



sclf.fit(X_train, y_train)
predictions = sclf.predict(X_test)
print(accuracy_score(y_test, predictions))
print(confusion_matrix(y_test, predictions))
print(classification_report(y_test, predictions))


print('3-fold cross validation:\n')

for clf, label in zip([clf1, clf2, clf3, clf4, sclf], 
                      ['DT', 
                       'ET', 
                       'RF',
                       'GB',
                       'StackingClassifier']):

    scores = model_selection.cross_val_score(clf, X_train, y_train, 
                                              cv=3, scoring='accuracy')
    print("Accuracy: %0.2f (+/- %0.2f) [%s]" 
          % (scores.mean(), scores.std(), label))
'''